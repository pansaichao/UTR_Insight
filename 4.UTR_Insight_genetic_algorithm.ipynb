{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import esm\n",
    "from esm.data import *\n",
    "from esm.model.esm2_secondarystructure import ESM2 as ESM2_SISS\n",
    "from esm.model.esm2_supervised import ESM2\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from esm.modules import ConvTransformerLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from collections import Counter\n",
    "os.chdir('/devdata/pansc/github/')\n",
    "\n",
    "global layers, heads, embed_dim, batch_toks, cnn_layers, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 6\n",
    "heads = 16\n",
    "embed_dim = 128\n",
    "# batch_toks = 4096*2 #4096\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "repr_layers = [0, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformerPredictor(nn.Module):\n",
    "    def __init__(self, alphabet, dropout=0.2, CovTransformer_layers=3, \n",
    "                 kmer=7, layers=6, embed_dim=128, nodes=40, heads=16):\n",
    "        super(ConvTransformerPredictor, self).__init__()\n",
    "        self.embedding_size = embed_dim\n",
    "        self.nodes = nodes\n",
    "        self.dropout = dropout\n",
    "        self.esm2 = ESM2_SISS(num_layers = layers,\n",
    "                        embed_dim = embed_dim,\n",
    "                        attention_heads = heads,\n",
    "                        alphabet = alphabet) \n",
    "        # 修改为 nn.ModuleList\n",
    "        self.convtransformer_decoder = nn.ModuleList([\n",
    "            ConvTransformerLayer(embed_dim, embed_dim*4, heads, kmer-i*2, dropout=self.dropout, use_esm1b_layer_norm=True) #(kmer-i*2)\n",
    "            for i in range(CovTransformer_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 处理实验来源信息的线性层\n",
    "        self.experiment_dense = nn.Linear(2, self.nodes)  # 处理 one-hot 实验指示符\n",
    "        self.linear = nn.Linear(in_features = 6 * embed_dim, out_features = self.nodes)\n",
    "        self.linear_2 = nn.Linear(in_features = self.nodes, out_features = self.nodes * 4)\n",
    "        self.linear_3 = nn.Linear(in_features = self.nodes * 4, out_features = self.nodes)\n",
    "        self.output = nn.Linear(in_features = self.nodes, out_features = 1)\n",
    "\n",
    "    def forward(self, tokens, experiment_indicator, self_attn_padding_mask=None):\n",
    "        # ESM embedding\n",
    "        embeddings = self.esm2(tokens, repr_layers, return_representation=True)\n",
    "        embeddings_rep = embeddings[\"representations\"][layers][:, 1 : -1] #B*(T+2)*E -> B*T*E\n",
    "\n",
    "        for i, layer in enumerate(self.convtransformer_decoder):\n",
    "            x_o, attn = layer(x=embeddings_rep, self_attn_padding_mask=self_attn_padding_mask)  #tokens: B*T*E, x_o: B*T*E\n",
    "\n",
    "        x = torch.flip(x_o, dims=[1])  # Reverse along the sequence length dimension\n",
    "        # Select frames corresponding to frame 1, frame 2, and frame 3\n",
    "        frame_1 = x[:, 0::3, :]\n",
    "        frame_2 = x[:, 1::3, :]\n",
    "        frame_3 = x[:, 2::3, :]\n",
    "        # 全局最大池化\n",
    "        frame_1_max = torch.max(frame_1, dim=1)[0]  # B*C\n",
    "        frame_2_max = torch.max(frame_2, dim=1)[0]  # B*C\n",
    "        frame_3_max = torch.max(frame_3, dim=1)[0]  # B*C\n",
    "        # 扩展 self_attn_padding_mask 的维度以匹配特征张量\n",
    "        mask_expanded = ~self_attn_padding_mask.unsqueeze(2)  # (batch_size, seq_len, 1)，True 表示有效数据\n",
    "        # 计算有效位置的均值池化\n",
    "        def masked_mean(frame, mask):\n",
    "            frame_sum = torch.sum(frame * mask, dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1) + 1e-8  # 避免除零\n",
    "            return frame_sum / mask_sum\n",
    "        # 全局均值池化\n",
    "        frame_1_avg = masked_mean(frame_1, mask_expanded[:, 0::3, :])\n",
    "        frame_2_avg = masked_mean(frame_2, mask_expanded[:, 1::3, :])\n",
    "        frame_3_avg = masked_mean(frame_3, mask_expanded[:, 2::3, :])\n",
    "        # 将池化后的张量拼接为一个张量\n",
    "        pooled_output = torch.cat([frame_1_max, frame_1_avg, frame_2_max, frame_2_avg, frame_3_max, frame_3_avg], dim=1)  # B*(6*C)\n",
    "        # 线性层处理实验指示符\n",
    "        experiment_output = self.experiment_dense(experiment_indicator)\n",
    "        x_pooled = self.flatten(pooled_output)\n",
    "\n",
    "        o_linear = self.linear(x_pooled) + experiment_output #将池化输出与实验信息拼接\n",
    "        o_linear_2 = self.linear_2(o_linear)\n",
    "        o_linear_3 = self.linear_3(o_linear_2)\n",
    "\n",
    "        o_relu = self.relu(o_linear_3)\n",
    "        o_dropout = self.dropout(o_relu)\n",
    "        o = self.output(o_dropout)  # B*1\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def performances(label, pred):\n",
    "    label, pred = list(label), list(pred)\n",
    "    r = r2(label, pred)\n",
    "    R2 = r2_score(label, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(label, pred))\n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    try:\n",
    "        pearson_r = pearsonr(label, pred)[0]\n",
    "    except:\n",
    "        pearson_r = -1e-9\n",
    "    try:\n",
    "        sp_cor = spearmanr(label, pred)[0]\n",
    "    except:\n",
    "        sp_cor = -1e-9\n",
    "    print(f'r-squared = {r:.4f} | pearson r = {pearson_r:.4f} | spearman R = {sp_cor:.4f} | R-squared = {R2:.4f} | RMSE = {rmse:.4f} | MAE = {mae:.4f}')\n",
    "    return [r, pearson_r, sp_cor, R2, rmse, mae]\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    performances_pd = pd.DataFrame(performances_list, index = ['r2', 'PearsonR', 'SpearmanR', 'R2', 'RMSE', 'MAE']).T\n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dataloader(e_data, obj_col, lab_col, batch_toks=8192*4, mask_prob = 0.0):\n",
    "    dataset = FastaBatchedDataset(e_data.loc[:,obj_col], e_data.loc[:, lab_col], mask_prob = mask_prob)\n",
    "    batches = dataset.get_batch_indices(toks_per_batch=batch_toks, extra_toks_per_seq=2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            collate_fn=alphabet.get_batch_converter(), \n",
    "                                            batch_sampler=batches, \n",
    "                                            shuffle = False)\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    return dataset, dataloader, batches\n",
    "\n",
    "def get_experiment_indicator_for_batch(data_combine, batch_idx):\n",
    "    # 从 train_combine 中获取对应 batch 的 experiment_indicator\n",
    "    batch_experiment_indicators = data_combine.iloc[batch_idx]['experiment_indicator'].values.tolist()\n",
    "    # 转换为 tensor\n",
    "    experiment_indicator_tensor = torch.tensor(batch_experiment_indicators, dtype=torch.float32).to(device)\n",
    "    return experiment_indicator_tensor\n",
    "\n",
    "def shuffle_data_fn(in_data):\n",
    "    # 使用 sample(frac=1) 来打乱数据集顺序\n",
    "    shuffle_data = in_data.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, train_shuffle_combine, train_shuffle_batch, model, epoch):        \n",
    "    model.train()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    \n",
    "    for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        toks = toks.to(device)\n",
    "        padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "        labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "        experiment_indicator_tensor = get_experiment_indicator_for_batch(train_shuffle_combine, train_shuffle_batch[i])\n",
    "\n",
    "        outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach())\n",
    "\n",
    "        y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "        y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "        y_pred_list.extend(y_pred)\n",
    "        \n",
    "    loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "    print(f'Train: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "    \n",
    "    metrics = performances(y_true_list, y_pred_list)\n",
    "    return metrics, loss_epoch\n",
    "\n",
    "def eval_step(test_dataloader, test_combine, test_batch, model, epoch):\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    strs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(test_dataloader)):\n",
    "            strs_list.extend(strs)\n",
    "            toks = toks.to(device)\n",
    "            padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "            labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "            experiment_indicator_tensor = get_experiment_indicator_for_batch(test_combine, test_batch[i])\n",
    "            \n",
    "            outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.cpu().detach())\n",
    "\n",
    "            y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "            y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "            y_pred_list.extend(y_pred)\n",
    "        \n",
    "        loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "        print(f'Test: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "        metrics = performances(y_true_list, y_pred_list)\n",
    "        e_pred = pd.DataFrame([strs_list, y_true_list, y_pred_list], index = ['utr', 'y_true', 'y_pred']).T\n",
    "        \n",
    "    return metrics, loss_epoch, e_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = Alphabet(mask_prob = 0.0, standard_toks = 'AGCT')\n",
    "print(alphabet.tok_to_idx)\n",
    "assert alphabet.tok_to_idx == {'<pad>': 0, '<eos>': 1, '<unk>': 2, 'A': 3, 'G': 4, 'C': 5, 'T': 6, '<cls>': 7, '<mask>': 8, '<sep>': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_modelfile = '/devdata/pansc/github/UTR_Insight/Model/utr_lm/ESM2SISS_FS4.22_fiveSpeciesCao_6layers_16heads_128embedsize_4096batchToks_lr1e-05_supervisedweight1.0_structureweight1.0_MLMLossMin_epoch115.pkl'\n",
    "model = ConvTransformerPredictor(alphabet).to(device)\n",
    "state_dict = torch.load(esm2_modelfile, map_location=device)\n",
    "model.esm2.load_state_dict({k.replace('module.', ''):v for k,v in state_dict.items()})\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "learning_rate = 1e-4 #1e-4, 1e-05\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=learning_rate,\n",
    "#     betas = (0.9, 0.999),\n",
    "#     eps = 1e-08\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate, \n",
    "    momentum=0.9,\n",
    "    weight_decay = 1e-4)\n",
    "\n",
    "# criterion = torch.nn.MSELoss() #torch.nn.HuberLoss()\n",
    "criterion = torch.nn.HuberLoss()\n",
    "\n",
    "loss_best, ep_best, r2_best = np.inf, -1, -1\n",
    "loss_train_dict, loss_test_dict = dict(), dict()\n",
    "\n",
    "metrics_train_dict = dict()\n",
    "metrics_test_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 UTR 序列的字符集为 'A', 'G', 'C', 'T'\n",
    "alphabet_set = ['A', 'G', 'C', 'T']\n",
    "\n",
    "# 初始化种群，生成若干随机序列\n",
    "def initialize_population(pop_size, seq_len):\n",
    "    population = [''.join(random.choices(alphabet_set, k=seq_len)) for _ in range(pop_size)]\n",
    "    return population\n",
    "\n",
    "# 适应度函数：通过模型评估序列的表现\n",
    "def fitness_function(sequence, model, device):\n",
    "    # 将 sequence 转换为模型所需的格式\n",
    "    input_tensor = torch.tensor([[alphabet.tok_to_idx[nuc] for nuc in sequence]], dtype=torch.long).to(device)\n",
    "    experiment_indicator = torch.tensor([[0.0, 1.0]], dtype=torch.float32).to(device)  # 假设使用默认的实验指示符\n",
    "    padding_mask = input_tensor.eq(alphabet.padding_idx)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, experiment_indicator, self_attn_padding_mask=padding_mask)\n",
    "    \n",
    "    # 假设模型输出是 MRL 的预测值\n",
    "    rl_prediction = output.item()\n",
    "    return rl_prediction\n",
    "\n",
    "# 选择：根据适应度选择父代\n",
    "def select_parents(population, fitness_scores, num_parents):\n",
    "    parents_idx = np.argsort(fitness_scores)[-num_parents:]  # 选择得分最高的个体\n",
    "    return [population[i] for i in parents_idx]\n",
    "\n",
    "# 交叉：生成新的子代\n",
    "def crossover(parents, crossover_rate=0.5):\n",
    "    new_population = []\n",
    "    for i in range(0, len(parents), 2):\n",
    "        if i + 1 < len(parents):\n",
    "            parent1, parent2 = parents[i], parents[i + 1]\n",
    "            if random.random() < crossover_rate:\n",
    "                split_point = random.randint(0, len(parent1))\n",
    "                child1 = parent1[:split_point] + parent2[split_point:]\n",
    "                child2 = parent2[:split_point] + parent1[split_point:]\n",
    "                new_population.extend([child1, child2])\n",
    "            else:\n",
    "                new_population.extend([parent1, parent2])\n",
    "    return new_population\n",
    "\n",
    "# 变异：对子代进行随机变异\n",
    "def mutate(sequence, mutation_rate=0.01):\n",
    "    sequence_list = list(sequence)\n",
    "    for i in range(len(sequence_list)):\n",
    "        if random.random() < mutation_rate:\n",
    "            sequence_list[i] = random.choice(alphabet_set)\n",
    "    return ''.join(sequence_list)\n",
    "\n",
    "# 遗传算法主函数\n",
    "def genetic_algorithm(model, device, pop_size=100, seq_len=100, num_generations=50, num_parents=20, mutation_rate=0.01, crossover_rate=0.5):\n",
    "    population = initialize_population(pop_size, seq_len)\n",
    "    \n",
    "    for generation in range(num_generations):\n",
    "        # 评估当前种群的适应度\n",
    "        fitness_scores = [fitness_function(seq, model, device) for seq in population]\n",
    "        \n",
    "        # 输出当前最优解\n",
    "        best_idx = np.argmax(fitness_scores)\n",
    "        print(f\"Generation {generation}: Best sequence = {population[best_idx]}, Best fitness = {fitness_scores[best_idx]}\")\n",
    "        \n",
    "        # 选择父代\n",
    "        parents = select_parents(population, fitness_scores, num_parents)\n",
    "        \n",
    "        # 交叉生成新的子代\n",
    "        offspring = crossover(parents, crossover_rate=crossover_rate)\n",
    "        \n",
    "        # 变异\n",
    "        offspring = [mutate(seq, mutation_rate=mutation_rate) for seq in offspring]\n",
    "        \n",
    "        # 更新种群，使用父代和子代一起构成新的种群\n",
    "        population = parents + offspring\n",
    "    \n",
    "    # 返回最终最优的序列\n",
    "    return population[best_idx]\n",
    "\n",
    "# 设置模型和设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvTransformerPredictor(alphabet).to(device)\n",
    "\n",
    "# 加载模型参数\n",
    "state_dict = torch.load(esm2_modelfile, map_location=device)\n",
    "model.esm2.load_state_dict({k.replace('module.', ''): v for k, v in state_dict.items()})\n",
    "\n",
    "# 运行遗传算法，优化序列\n",
    "best_sequence = genetic_algorithm(model, device, pop_size=100, seq_len=50, num_generations=10000, num_parents=20, mutation_rate=0.01, crossover_rate=0.5)\n",
    "print(f\"Optimized UTR sequence: {best_sequence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
