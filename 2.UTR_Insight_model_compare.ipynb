{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import esm\n",
    "from esm.data import *\n",
    "from esm.model.esm2_secondarystructure import ESM2 as ESM2_SISS\n",
    "from esm.model.esm2_supervised import ESM2\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from esm.modules import ConvTransformerLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from collections import Counter\n",
    "os.chdir('/devdata/pansc/github/')\n",
    "\n",
    "global layers, heads, embed_dim, batch_toks, cnn_layers, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 6\n",
    "heads = 16\n",
    "embed_dim = 128\n",
    "# batch_toks = 4096*2 #4096\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "repr_layers = [0, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformerPredictor(nn.Module):\n",
    "    def __init__(self, alphabet, dropout=0.2, CovTransformer_layers=3, \n",
    "                 kmer=7, layers=6, embed_dim=128, nodes=40, heads=16):\n",
    "        super(ConvTransformerPredictor, self).__init__()\n",
    "        self.embedding_size = embed_dim\n",
    "        self.nodes = nodes\n",
    "        self.dropout = dropout\n",
    "        self.esm2 = ESM2_SISS(num_layers = layers,\n",
    "                        embed_dim = embed_dim,\n",
    "                        attention_heads = heads,\n",
    "                        alphabet = alphabet) \n",
    "        # 修改为 nn.ModuleList\n",
    "        self.convtransformer_decoder = nn.ModuleList([\n",
    "            ConvTransformerLayer(embed_dim, embed_dim*4, heads, kmer-i*2, dropout=self.dropout, use_esm1b_layer_norm=True) #(kmer-i*2)\n",
    "            for i in range(CovTransformer_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 处理实验来源信息的线性层\n",
    "        self.experiment_dense = nn.Linear(2, self.nodes)  # 处理 one-hot 实验指示符\n",
    "        self.linear = nn.Linear(in_features = 6 * embed_dim, out_features = self.nodes)\n",
    "        self.linear_2 = nn.Linear(in_features = self.nodes, out_features = self.nodes * 4)\n",
    "        self.linear_3 = nn.Linear(in_features = self.nodes * 4, out_features = self.nodes)\n",
    "        self.output = nn.Linear(in_features = self.nodes, out_features = 1)\n",
    "\n",
    "    def forward(self, tokens, experiment_indicator, self_attn_padding_mask=None):\n",
    "        # ESM embedding\n",
    "        embeddings = self.esm2(tokens, repr_layers, return_representation=True)\n",
    "        embeddings_rep = embeddings[\"representations\"][layers][:, 1 : -1] #B*(T+2)*E -> B*T*E\n",
    "\n",
    "        for i, layer in enumerate(self.convtransformer_decoder):\n",
    "            x_o, attn = layer(x=embeddings_rep, self_attn_padding_mask=self_attn_padding_mask)  #tokens: B*T*E, x_o: B*T*E\n",
    "\n",
    "        x = torch.flip(x_o, dims=[1])  # Reverse along the sequence length dimension\n",
    "        # Select frames corresponding to frame 1, frame 2, and frame 3\n",
    "        frame_1 = x[:, 0::3, :]\n",
    "        frame_2 = x[:, 1::3, :]\n",
    "        frame_3 = x[:, 2::3, :]\n",
    "        # 全局最大池化\n",
    "        frame_1_max = torch.max(frame_1, dim=1)[0]  # B*C\n",
    "        frame_2_max = torch.max(frame_2, dim=1)[0]  # B*C\n",
    "        frame_3_max = torch.max(frame_3, dim=1)[0]  # B*C\n",
    "        # 扩展 self_attn_padding_mask 的维度以匹配特征张量\n",
    "        mask_expanded = ~self_attn_padding_mask.unsqueeze(2)  # (batch_size, seq_len, 1)，True 表示有效数据\n",
    "        # 计算有效位置的均值池化\n",
    "        def masked_mean(frame, mask):\n",
    "            frame_sum = torch.sum(frame * mask, dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1) + 1e-8  # 避免除零\n",
    "            return frame_sum / mask_sum\n",
    "        # 全局均值池化\n",
    "        frame_1_avg = masked_mean(frame_1, mask_expanded[:, 0::3, :])\n",
    "        frame_2_avg = masked_mean(frame_2, mask_expanded[:, 1::3, :])\n",
    "        frame_3_avg = masked_mean(frame_3, mask_expanded[:, 2::3, :])\n",
    "        # 将池化后的张量拼接为一个张量\n",
    "        pooled_output = torch.cat([frame_1_max, frame_1_avg, frame_2_max, frame_2_avg, frame_3_max, frame_3_avg], dim=1)  # B*(6*C)\n",
    "        # 线性层处理实验指示符\n",
    "        experiment_output = self.experiment_dense(experiment_indicator)\n",
    "        x_pooled = self.flatten(pooled_output)\n",
    "\n",
    "        o_linear = self.linear(x_pooled) + experiment_output #将池化输出与实验信息拼接\n",
    "        o_linear_2 = self.linear_2(o_linear)\n",
    "        o_linear_3 = self.linear_3(o_linear_2)\n",
    "\n",
    "        o_relu = self.relu(o_linear_3)\n",
    "        o_dropout = self.dropout(o_relu)\n",
    "        o = self.output(o_dropout)  # B*1\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def performances(label, pred):\n",
    "    label, pred = list(label), list(pred)\n",
    "    r = r2(label, pred)\n",
    "    R2 = r2_score(label, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(label, pred))\n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    try:\n",
    "        pearson_r = pearsonr(label, pred)[0]\n",
    "    except:\n",
    "        pearson_r = -1e-9\n",
    "    try:\n",
    "        sp_cor = spearmanr(label, pred)[0]\n",
    "    except:\n",
    "        sp_cor = -1e-9\n",
    "    print(f'r-squared = {r:.4f} | pearson r = {pearson_r:.4f} | spearman R = {sp_cor:.4f} | R-squared = {R2:.4f} | RMSE = {rmse:.4f} | MAE = {mae:.4f}')\n",
    "    return [r, pearson_r, sp_cor, R2, rmse, mae]\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    performances_pd = pd.DataFrame(performances_list, index = ['r2', 'PearsonR', 'SpearmanR', 'R2', 'RMSE', 'MAE']).T\n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dataloader(e_data, obj_col, lab_col, batch_toks=8192, mask_prob = 0.0):\n",
    "    dataset = FastaBatchedDataset(e_data.loc[:,obj_col], e_data.loc[:, lab_col], mask_prob = mask_prob)\n",
    "    batches = dataset.get_batch_indices(toks_per_batch=batch_toks, extra_toks_per_seq=2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            collate_fn=alphabet.get_batch_converter(), \n",
    "                                            batch_sampler=batches, \n",
    "                                            shuffle = False)\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    return dataset, dataloader, batches\n",
    "\n",
    "def get_experiment_indicator_for_batch(data_combine, batch_idx):\n",
    "    # 从 train_combine 中获取对应 batch 的 experiment_indicator\n",
    "    batch_experiment_indicators = data_combine.iloc[batch_idx]['experiment_indicator'].values.tolist()\n",
    "    # 转换为 tensor\n",
    "    experiment_indicator_tensor = torch.tensor(batch_experiment_indicators, dtype=torch.float32).to(device)\n",
    "    return experiment_indicator_tensor\n",
    "\n",
    "def shuffle_data_fn(in_data):\n",
    "    # 使用 sample(frac=1) 来打乱数据集顺序\n",
    "    shuffle_data = in_data.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, train_shuffle_combine, train_shuffle_batch, model, epoch):        \n",
    "    model.train()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    \n",
    "    for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        toks = toks.to(device)\n",
    "        padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "        labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "        experiment_indicator_tensor = get_experiment_indicator_for_batch(train_shuffle_combine, train_shuffle_batch[i])\n",
    "\n",
    "        outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach())\n",
    "\n",
    "        y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "        y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "        y_pred_list.extend(y_pred)\n",
    "        \n",
    "    loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "    print(f'Train: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "    \n",
    "    metrics = performances(y_true_list, y_pred_list)\n",
    "    return metrics, loss_epoch\n",
    "\n",
    "def eval_step(test_dataloader, test_combine, test_batch, model, epoch):\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    strs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(test_dataloader)):\n",
    "            strs_list.extend(strs)\n",
    "            toks = toks.to(device)\n",
    "            padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "            labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "            experiment_indicator_tensor = get_experiment_indicator_for_batch(test_combine, test_batch[i])\n",
    "            \n",
    "            outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.cpu().detach())\n",
    "\n",
    "            y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "            y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "            y_pred_list.extend(y_pred)\n",
    "        \n",
    "        loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "        print(f'Test: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "        metrics = performances(y_true_list, y_pred_list)\n",
    "        e_pred = pd.DataFrame([strs_list, y_true_list, y_pred_list], index = ['utr', 'y_true', 'y_pred']).T\n",
    "        \n",
    "    return metrics, loss_epoch, e_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_mrl_random_50 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/optimus_framepool/4.1_test_data_GSM3130435_egfp_unmod_1_pred_mrl.csv')\n",
    "cnn_pred_mrl_human_100 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/optimus_framepool/VaryLengthHumanTest_sequence_num7600_pred_mrl.csv')\n",
    "cnn_pred_mrl_random_100 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/optimus_framepool/VaryLengthRandomTest_sequence_num7600_pred_mrl.csv')\n",
    "\n",
    "lm_pred_mrl_random_50 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/utr_insight/e_pred_random_50.csv')\n",
    "lm_pred_mrl_human_100 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/utr_insight/e_pred_human_100.csv')\n",
    "lm_pred_mrl_random_100 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/utr_insight/e_pred_random_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_pred_mrl_human_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results = [cnn_pred_mrl_random_50, cnn_pred_mrl_random_100, cnn_pred_mrl_human_100]\n",
    "lm_results = [lm_pred_mrl_random_50, lm_pred_mrl_random_100, lm_pred_mrl_human_100]\n",
    "\n",
    "var_prefix = ['random_50', 'random_100', 'human_100']\n",
    "var_prefix_vary = ['random_100', 'human_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化空列表存储每次循环生成的DataFrame\n",
    "all_results = []\n",
    "\n",
    "for i in range(len(cnn_results)):\n",
    "    rl = cnn_results[i].loc[:, 'rl']\n",
    "    rl_lm = lm_results[i].loc[:, 'y_true']\n",
    "    \n",
    "    optimus_pred = cnn_results[i].loc[:, 'pred_optimus']\n",
    "    framepool_pred = cnn_results[i].loc[:, 'pred_frame_pool']\n",
    "    lm_pred = lm_results[i].loc[:, 'y_pred']\n",
    "\n",
    "    optimus_per = performances(rl, optimus_pred)\n",
    "    framepool_per = performances(rl, framepool_pred)\n",
    "    lm_per = performances(rl_lm, lm_pred)\n",
    "\n",
    "    optimus_pd = performances_to_pd(optimus_per)\n",
    "    framepool_pd = performances_to_pd(framepool_per)\n",
    "    lm_pd= performances_to_pd(lm_per)\n",
    "\n",
    "    # 为每个DataFrame添加“数据集”和“模型”列\n",
    "    optimus_pd['Dataset'] = var_prefix[i]\n",
    "    optimus_pd['Model'] = 'optimus'\n",
    "    \n",
    "    framepool_pd['Dataset'] = var_prefix[i]\n",
    "    framepool_pd['Model'] = 'framepool'\n",
    "\n",
    "    lm_pd['Dataset'] = var_prefix[i]\n",
    "    lm_pd['Model'] = 'utr_insignt'\n",
    "    \n",
    "    # 将每个DataFrame存储到all_results列表中\n",
    "    all_results.append(optimus_pd)\n",
    "    all_results.append(framepool_pd)\n",
    "    all_results.append(lm_pd)\n",
    "\n",
    "# 将所有的DataFrame拼接成一个大DataFrame\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前的列名列表\n",
    "cols = final_df.columns.tolist()\n",
    "new_order = ['Model', 'Dataset'] + [col for col in cols if col not in ['Dataset', 'Model']]\n",
    "final_df = final_df[new_order]\n",
    "# 打印结果以检查\n",
    "# final_df.to_csv('/devdata/pansc/github/UTR-LM/Scripts/UTRLM_downstream/model_compare.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "utr_lm_pred_mrl_random_50 = pd.read_csv('/devdata/pansc/github/UTR_Insight/Result/utr_lm/UTR_LM_prediction_random_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "utr_lm_rl = utr_lm_pred_mrl_random_50.loc[:, 'rl']\n",
    "utr_lm_pred = utr_lm_pred_mrl_random_50.loc[:, 'MRL']\n",
    "utr_lm_per = performances(utr_lm_rl, utr_lm_pred)\n",
    "utr_lm_pd= performances_to_pd(utr_lm_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results_new = []\n",
    "\n",
    "# 遍历cnn_results数组并更新每个DataFrame\n",
    "for i in range(len(cnn_results)):\n",
    "    # 提取需要的列\n",
    "    cnn_results[i] = cnn_results[i][['id', 'utr', 'rl', 'pred_optimus', 'pred_frame_pool']]\n",
    "    # 根据utr列计算序列长度，并添加新列'len'\n",
    "    cnn_results[i]['len'] = cnn_results[i]['utr'].apply(len)\n",
    "    cnn_results_new.append(cnn_results[i])\n",
    "\n",
    "# 初始化一个新的列表存储处理后的DataFrame\n",
    "lm_results_new = []\n",
    "\n",
    "# 遍历lm_results数组并更新每个DataFrame\n",
    "for df in lm_results:\n",
    "    # 复制DataFrame，避免直接修改原数据\n",
    "    df_new = df.copy() \n",
    "    # 去除utr列中的'<pad>'并存储为新的列raw_utr\n",
    "    df_new['raw_utr'] = df_new['utr'].str.replace('<pad>', '', regex=False)\n",
    "    df_new['len'] = df_new['raw_utr'].apply(len)\n",
    "    # 将处理后的DataFrame存储到lm_results_new列表中\n",
    "    lm_results_new.append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results_vary = cnn_results_new[1:]\n",
    "lm_results_vary = lm_results_new[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义序列长度的分段区间\n",
    "length_bins = [(25, 44), (45, 64), (65, 84), (85, 100)]\n",
    "length_labels = ['25-44', '45-64', '65-84', '85-100']\n",
    "\n",
    "all_results_grpup = []\n",
    "for i, (min_len, max_len) in enumerate(length_bins):\n",
    "    for j in range(len(cnn_results_vary)):\n",
    "\n",
    "        cnn_results_vary_group = cnn_results_vary[j][(cnn_results_vary[j]['len'] >= min_len) & (cnn_results_vary[j]['len'] <= max_len)]\n",
    "        lm_results_vary_group = lm_results_vary[j][(lm_results_vary[j]['len'] >= min_len) & (lm_results_vary[j]['len'] <= max_len)]\n",
    "\n",
    "        rl = cnn_results_vary_group.loc[:, 'rl']\n",
    "        rl_lm = lm_results_vary_group.loc[:, 'y_true']\n",
    "\n",
    "        optimus_pred = cnn_results_vary_group.loc[:, 'pred_optimus']\n",
    "        framepool_pred = cnn_results_vary_group.loc[:, 'pred_frame_pool']\n",
    "        lm_pred = lm_results_vary_group.loc[:, 'y_pred']\n",
    "\n",
    "        optimus_per = performances(rl, optimus_pred)\n",
    "        framepool_per = performances(rl, framepool_pred)\n",
    "        lm_per = performances(rl_lm, lm_pred)\n",
    "\n",
    "        optimus_pd = performances_to_pd(optimus_per)\n",
    "        framepool_pd = performances_to_pd(framepool_per)\n",
    "        lm_pd= performances_to_pd(lm_per)\n",
    "\n",
    "        # 为每个DataFrame添加“数据集”和“模型”列和“长度分段”列\n",
    "        optimus_pd['Dataset'] = var_prefix_vary[j]\n",
    "        optimus_pd['Model'] = 'optimus'\n",
    "        optimus_pd['group'] = length_labels[i]\n",
    "        \n",
    "        framepool_pd['Dataset'] = var_prefix_vary[j]\n",
    "        framepool_pd['Model'] = 'framepool'\n",
    "        framepool_pd['group'] = length_labels[i]\n",
    "\n",
    "        lm_pd['Dataset'] = var_prefix_vary[j]\n",
    "        lm_pd['Model'] = 'utr_insignt'\n",
    "        lm_pd['group'] = length_labels[i]\n",
    "\n",
    "        all_results_grpup.append(optimus_pd)\n",
    "        all_results_grpup.append(framepool_pd)\n",
    "        all_results_grpup.append(lm_pd)\n",
    "\n",
    "# 将所有的DataFrame拼接成一个大DataFrame\n",
    "final_df_group = pd.concat(all_results_grpup, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_group.to_csv('/devdata/pansc/github/UTR_Insight/Result/model_compare/model_compare_group.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
