{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import esm\n",
    "from esm.data import *\n",
    "from esm.model.esm2_secondarystructure import ESM2 as ESM2_SISS\n",
    "from esm.model.esm2_supervised import ESM2\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from esm.modules import ConvTransformerLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from collections import Counter\n",
    "os.chdir('/devdata/pansc/github/')\n",
    "\n",
    "global layers, heads, embed_dim, batch_toks, cnn_layers, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 6\n",
    "heads = 16\n",
    "embed_dim = 128\n",
    "# batch_toks = 4096*2 #4096\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "repr_layers = [0, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformerPredictor(nn.Module):\n",
    "    def __init__(self, alphabet, dropout=0.2, CovTransformer_layers=3, \n",
    "                 kmer=7, layers=6, embed_dim=128, nodes=40, heads=16):\n",
    "        super(ConvTransformerPredictor, self).__init__()\n",
    "        self.embedding_size = embed_dim\n",
    "        self.nodes = nodes\n",
    "        self.dropout = dropout\n",
    "        self.esm2 = ESM2_SISS(num_layers = layers,\n",
    "                        embed_dim = embed_dim,\n",
    "                        attention_heads = heads,\n",
    "                        alphabet = alphabet) \n",
    "        # 修改为 nn.ModuleList\n",
    "        self.convtransformer_decoder = nn.ModuleList([\n",
    "            ConvTransformerLayer(embed_dim, embed_dim*4, heads, kmer-i*2, dropout=self.dropout, use_esm1b_layer_norm=True) #(kmer-i*2)\n",
    "            for i in range(CovTransformer_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 处理实验来源信息的线性层\n",
    "        self.experiment_dense = nn.Linear(2, self.nodes)  # 处理 one-hot 实验指示符\n",
    "        self.linear = nn.Linear(in_features = 6 * embed_dim, out_features = self.nodes)\n",
    "        self.linear_2 = nn.Linear(in_features = self.nodes, out_features = self.nodes * 4)\n",
    "        self.linear_3 = nn.Linear(in_features = self.nodes * 4, out_features = self.nodes)\n",
    "        self.output = nn.Linear(in_features = self.nodes, out_features = 1)\n",
    "\n",
    "    def forward(self, tokens, experiment_indicator, self_attn_padding_mask=None):\n",
    "        # ESM embedding\n",
    "        embeddings = self.esm2(tokens, repr_layers, return_representation=True)\n",
    "        embeddings_rep = embeddings[\"representations\"][layers][:, 1 : -1] #B*(T+2)*E -> B*T*E\n",
    "\n",
    "        for i, layer in enumerate(self.convtransformer_decoder):\n",
    "            x_o, attn = layer(x=embeddings_rep, self_attn_padding_mask=self_attn_padding_mask)  #tokens: B*T*E, x_o: B*T*E\n",
    "\n",
    "        x = torch.flip(x_o, dims=[1])  # Reverse along the sequence length dimension\n",
    "        # Select frames corresponding to frame 1, frame 2, and frame 3\n",
    "        frame_1 = x[:, 0::3, :]\n",
    "        frame_2 = x[:, 1::3, :]\n",
    "        frame_3 = x[:, 2::3, :]\n",
    "        # 全局最大池化\n",
    "        frame_1_max = torch.max(frame_1, dim=1)[0]  # B*C\n",
    "        frame_2_max = torch.max(frame_2, dim=1)[0]  # B*C\n",
    "        frame_3_max = torch.max(frame_3, dim=1)[0]  # B*C\n",
    "        # 扩展 self_attn_padding_mask 的维度以匹配特征张量\n",
    "        mask_expanded = ~self_attn_padding_mask.unsqueeze(2)  # (batch_size, seq_len, 1)，True 表示有效数据\n",
    "        # 计算有效位置的均值池化\n",
    "        def masked_mean(frame, mask):\n",
    "            frame_sum = torch.sum(frame * mask, dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1) + 1e-8  # 避免除零\n",
    "            return frame_sum / mask_sum\n",
    "        # 全局均值池化\n",
    "        frame_1_avg = masked_mean(frame_1, mask_expanded[:, 0::3, :])\n",
    "        frame_2_avg = masked_mean(frame_2, mask_expanded[:, 1::3, :])\n",
    "        frame_3_avg = masked_mean(frame_3, mask_expanded[:, 2::3, :])\n",
    "        # 将池化后的张量拼接为一个张量\n",
    "        pooled_output = torch.cat([frame_1_max, frame_1_avg, frame_2_max, frame_2_avg, frame_3_max, frame_3_avg], dim=1)  # B*(6*C)\n",
    "        # 线性层处理实验指示符\n",
    "        experiment_output = self.experiment_dense(experiment_indicator)\n",
    "        x_pooled = self.flatten(pooled_output)\n",
    "\n",
    "        o_linear = self.linear(x_pooled) + experiment_output #将池化输出与实验信息拼接\n",
    "        o_linear_2 = self.linear_2(o_linear)\n",
    "        o_linear_3 = self.linear_3(o_linear_2)\n",
    "\n",
    "        o_relu = self.relu(o_linear_3)\n",
    "        o_dropout = self.dropout(o_relu)\n",
    "        o = self.output(o_dropout)  # B*1\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def performances(label, pred):\n",
    "    label, pred = list(label), list(pred)\n",
    "    r = r2(label, pred)\n",
    "    R2 = r2_score(label, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(label, pred))\n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    try:\n",
    "        pearson_r = pearsonr(label, pred)[0]\n",
    "    except:\n",
    "        pearson_r = -1e-9\n",
    "    try:\n",
    "        sp_cor = spearmanr(label, pred)[0]\n",
    "    except:\n",
    "        sp_cor = -1e-9\n",
    "    print(f'r-squared = {r:.4f} | pearson r = {pearson_r:.4f} | spearman R = {sp_cor:.4f} | R-squared = {R2:.4f} | RMSE = {rmse:.4f} | MAE = {mae:.4f}')\n",
    "    return [r, pearson_r, sp_cor, R2, rmse, mae]\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    performances_pd = pd.DataFrame(performances_list, index = ['r2', 'PearsonR', 'SpearmanR', 'R2', 'RMSE', 'MAE']).T\n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dataloader(e_data, obj_col, lab_col, batch_toks=8192, mask_prob = 0.0):\n",
    "    dataset = FastaBatchedDataset(e_data.loc[:,obj_col], e_data.loc[:, lab_col], mask_prob = mask_prob)\n",
    "    batches = dataset.get_batch_indices(toks_per_batch=batch_toks, extra_toks_per_seq=2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            collate_fn=alphabet.get_batch_converter(), \n",
    "                                            batch_sampler=batches, \n",
    "                                            shuffle = False)\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    return dataset, dataloader, batches\n",
    "\n",
    "def get_experiment_indicator_for_batch(data_combine, batch_idx):\n",
    "    # 从 train_combine 中获取对应 batch 的 experiment_indicator\n",
    "    batch_experiment_indicators = data_combine.iloc[batch_idx]['experiment_indicator'].values.tolist()\n",
    "    # 转换为 tensor\n",
    "    experiment_indicator_tensor = torch.tensor(batch_experiment_indicators, dtype=torch.float32).to(device)\n",
    "    return experiment_indicator_tensor\n",
    "\n",
    "def shuffle_data_fn(in_data):\n",
    "    # 使用 sample(frac=1) 来打乱数据集顺序\n",
    "    shuffle_data = in_data.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, train_shuffle_combine, train_shuffle_batch, model, epoch):        \n",
    "    model.train()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    \n",
    "    for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        toks = toks.to(device)\n",
    "        padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "        labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "        experiment_indicator_tensor = get_experiment_indicator_for_batch(train_shuffle_combine, train_shuffle_batch[i])\n",
    "\n",
    "        outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach())\n",
    "\n",
    "        y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "        y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "        y_pred_list.extend(y_pred)\n",
    "        \n",
    "    loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "    print(f'Train: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "    \n",
    "    metrics = performances(y_true_list, y_pred_list)\n",
    "    return metrics, loss_epoch\n",
    "\n",
    "def eval_step(test_dataloader, test_combine, test_batch, model, epoch):\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    strs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(test_dataloader)):\n",
    "            strs_list.extend(strs)\n",
    "            toks = toks.to(device)\n",
    "            padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "            labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "            experiment_indicator_tensor = get_experiment_indicator_for_batch(test_combine, test_batch[i])\n",
    "            \n",
    "            outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.cpu().detach())\n",
    "\n",
    "            y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "            y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "            y_pred_list.extend(y_pred)\n",
    "        \n",
    "        loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "        print(f'Test: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "        metrics = performances(y_true_list, y_pred_list)\n",
    "        e_pred = pd.DataFrame([strs_list, y_true_list, y_pred_list], index = ['utr', 'y_true', 'y_pred']).T\n",
    "        \n",
    "    return metrics, loss_epoch, e_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====Load Data====')\n",
    "endogenous_100 = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/endogenous/filtered_utr100_mrl_uorf.csv')\n",
    "endogenous_100_hm = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/endogenous/filtered_utr100_mrl_uorf_hm.csv')\n",
    "endogenous_selected = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/endogenous/RHTR0032-GFP更正_论文.csv')\n",
    "endogenous_denovo = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/endogenous/denovo_seq.csv')\n",
    "\n",
    "# 先将 utr100 列中的字符串转换为大写\n",
    "endogenous_100['utr100'] = endogenous_100['utr100'].str.upper()\n",
    "endogenous_100_hm['utr100'] = endogenous_100_hm['utr100'].str.upper()\n",
    "endogenous_selected['utr100'] = endogenous_selected['5utr'].str[7:].str.upper()\n",
    "endogenous_denovo['utr100'] = endogenous_denovo['utr'].str[7:].str.upper()\n",
    "\n",
    "# 先用 \"*\" 补全到 100 个字符，再将 \"*\" 替换为 \"<pad>\"\n",
    "endogenous_100['utr_100'] = endogenous_100['utr100'].str.pad(width=100, side='left', fillchar='*').str.replace('*', '<pad>')\n",
    "endogenous_100_hm['utr_100'] = endogenous_100_hm['utr100'].str.pad(width=100, side='left', fillchar='*').str.replace('*', '<pad>')\n",
    "endogenous_selected['utr_100'] = endogenous_selected['utr100'].str.pad(width=100, side='left', fillchar='*').str.replace('*', '<pad>')\n",
    "endogenous_denovo['utr_100'] = endogenous_denovo['utr100'].str.pad(width=100, side='left', fillchar='*').str.replace('*', '<pad>')\n",
    "\n",
    "# 选择 'rl' 和 'utr' 列\n",
    "endogenous_100_selected = endogenous_100[['pred_frame_pool_100', 'utr_100']]\n",
    "endogenous_100_hm_selected = endogenous_100_hm[['pred_frame_pool_100', 'utr_100']]\n",
    "endogenous_selected_1 = endogenous_selected[['pred_frame_pool', 'utr_100']]\n",
    "endogenous_denovo_2 = endogenous_denovo[['pred_frame_pool', 'utr_100']]\n",
    "\n",
    "# 为 train_data_50_selected 和 train_data_vary_selected 添加实验指示符\n",
    "# endogenous_100_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(endogenous_100_selected)\n",
    "# endogenous_100_hm_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(endogenous_100_hm_selected)\n",
    "# endogenous_selected_1.loc[:, 'experiment_indicator'] = [[0, 1]] * len(endogenous_selected_1)\n",
    "# endogenous_denovo_2.loc[:, 'experiment_indicator'] = [[0, 1]] * len(endogenous_denovo_2)\n",
    "\n",
    "endogenous_100_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(endogenous_100_selected)\n",
    "endogenous_100_hm_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(endogenous_100_hm_selected)\n",
    "endogenous_selected_1.loc[:, 'experiment_indicator'] = [[1, 0]] * len(endogenous_selected_1)\n",
    "endogenous_denovo_2.loc[:, 'experiment_indicator'] = [[1, 0]] * len(endogenous_denovo_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = Alphabet(mask_prob = 0.0, standard_toks = 'AGCT')\n",
    "print(alphabet.tok_to_idx)\n",
    "assert alphabet.tok_to_idx == {'<pad>': 0, '<eos>': 1, '<unk>': 2, 'A': 3, 'G': 4, 'C': 5, 'T': 6, '<cls>': 7, '<mask>': 8, '<sep>': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endogenous all\n",
    "endogenous_100_all, endogenous_100_all_dataloader, endogenous_100_all_batch = generate_dataset_dataloader(endogenous_100_selected, 'pred_frame_pool_100', 'utr_100', mask_prob = 0.0)\n",
    "# endogenous hm\n",
    "endogenous_100_hm, endogenous_100_hm_dataloader, endogenous_100_hm_batch = generate_dataset_dataloader(endogenous_100_hm_selected, 'pred_frame_pool_100', 'utr_100', mask_prob = 0.0)\n",
    "# endogenous_selected_1\n",
    "endogenous_selected_1_, endogenous_selected_1_dataloader, endogenous_selected_1_batch = generate_dataset_dataloader(endogenous_selected_1, 'pred_frame_pool', 'utr_100', mask_prob = 0.0)\n",
    "# endogenous_denovo_2\n",
    "endogenous_denovo_2_, endogenous_denovo_2_dataloader, endogenous_denovo_2_batch = generate_dataset_dataloader(endogenous_denovo_2, 'pred_frame_pool', 'utr_100', mask_prob = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '/devdata/pansc/github/UTR_Insight/Model/utr_insight/model_epoch199.pkl'\n",
    "model = ConvTransformerPredictor(alphabet).to(device)\n",
    "state_dict = torch.load(model_file, map_location=device)\n",
    "model.load_state_dict({k.replace('module.', ''):v for k,v in state_dict.items()})\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = 1e-4 #1e-4, 1e-05\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=learning_rate,\n",
    "#     betas = (0.9, 0.999),\n",
    "#     eps = 1e-08\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate, \n",
    "    momentum=0.9,\n",
    "    weight_decay = 1e-4)\n",
    "\n",
    "# criterion = torch.nn.MSELoss() #torch.nn.HuberLoss()\n",
    "criterion = torch.nn.HuberLoss()\n",
    "\n",
    "loss_best, ep_best, r2_best = np.inf, -1, -1\n",
    "loss_train_dict, loss_test_dict = dict(), dict()\n",
    "\n",
    "metrics_train_dict = dict()\n",
    "metrics_test_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endogenous_100_all test evaluation\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    metrics_eval, loss_eval, e_pred_endogenous_100_all = eval_step(endogenous_100_all_dataloader, endogenous_100_selected, endogenous_100_all_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_eval\n",
    "    metrics_test_dict[epoch] = metrics_eval\n",
    "\n",
    "# 将 e_pred_endogenous_100_all 保存为 CSV 文件\n",
    "e_pred_endogenous_100_all.to_csv('/devdata/pansc/github/UTR_Insight/Result/endogenous_1_0/e_pred_e_pred_endogenous_100_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endogenous_100_hm test evaluation\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    metrics_eval, loss_eval, e_pred_endogenous_100_hm = eval_step(endogenous_100_hm_dataloader, endogenous_100_hm_selected, endogenous_100_hm_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_eval\n",
    "    metrics_test_dict[epoch] = metrics_eval\n",
    "\n",
    "# 将 e_pred_endogenous_100_hm 保存为 CSV 文件\n",
    "e_pred_endogenous_100_hm.to_csv('/devdata/pansc/github/UTR_Insight/Result/endogenous_1_0/e_pred_endogenous_100_hm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endogenous_selected_1 test evaluation\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    metrics_eval, loss_eval, e_pred_endogenous_selected_1 = eval_step(endogenous_selected_1_dataloader, endogenous_selected_1, endogenous_selected_1_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_eval\n",
    "    metrics_test_dict[epoch] = metrics_eval\n",
    "\n",
    "# 将 e_pred_endogenous_selected_1 保存为 CSV 文件\n",
    "e_pred_endogenous_selected_1.to_csv('/devdata/pansc/github/UTR_Insight/Result/endogenous_1_0/e_pred_endogenous_selected_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endogenous_denovo_2 test evaluation\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    metrics_eval, loss_eval, e_pred_endogenous_denovo_2 = eval_step(endogenous_denovo_2_dataloader, endogenous_denovo_2, endogenous_denovo_2_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_eval\n",
    "    metrics_test_dict[epoch] = metrics_eval\n",
    "\n",
    "# 将 e_pred_endogenous_selected_1 保存为 CSV 文件\n",
    "e_pred_endogenous_denovo_2.to_csv('/devdata/pansc/github/UTR_Insight/Result/endogenous_1_0/e_pred_endogenous_denovo_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
