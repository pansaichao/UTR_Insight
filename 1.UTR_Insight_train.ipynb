{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import esm\n",
    "from esm.data import *\n",
    "from esm.model.esm2_secondarystructure import ESM2 as ESM2_SISS\n",
    "from esm.model.esm2_supervised import ESM2\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "from esm.modules import ConvTransformerLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from collections import Counter\n",
    "os.chdir('/devdata/pansc/github/')\n",
    "\n",
    "global layers, heads, embed_dim, batch_toks, cnn_layers, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 6\n",
    "heads = 16\n",
    "embed_dim = 128\n",
    "# batch_toks = 4096*2 #4096\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "repr_layers = [0, layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransformerPredictor(nn.Module):\n",
    "    def __init__(self, alphabet, dropout=0.2, CovTransformer_layers=3, \n",
    "                 kmer=7, layers=6, embed_dim=128, nodes=40, heads=16):\n",
    "        super(ConvTransformerPredictor, self).__init__()\n",
    "        self.embedding_size = embed_dim\n",
    "        self.nodes = nodes\n",
    "        self.dropout = dropout\n",
    "        self.esm2 = ESM2_SISS(num_layers = layers,\n",
    "                        embed_dim = embed_dim,\n",
    "                        attention_heads = heads,\n",
    "                        alphabet = alphabet) \n",
    "        # 修改为 nn.ModuleList\n",
    "        self.convtransformer_decoder = nn.ModuleList([\n",
    "            ConvTransformerLayer(embed_dim, embed_dim*4, heads, kmer-i*2, dropout=self.dropout, use_esm1b_layer_norm=True) #(kmer-i*2)\n",
    "            for i in range(CovTransformer_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 处理实验来源信息的线性层\n",
    "        self.experiment_dense = nn.Linear(2, self.nodes)  # 处理 one-hot 实验指示符\n",
    "        self.linear = nn.Linear(in_features = 6 * embed_dim, out_features = self.nodes)\n",
    "        self.linear_2 = nn.Linear(in_features = self.nodes, out_features = self.nodes * 4)\n",
    "        self.linear_3 = nn.Linear(in_features = self.nodes * 4, out_features = self.nodes)\n",
    "        self.output = nn.Linear(in_features = self.nodes, out_features = 1)\n",
    "\n",
    "    def forward(self, tokens, experiment_indicator, self_attn_padding_mask=None):\n",
    "        # ESM embedding\n",
    "        embeddings = self.esm2(tokens, repr_layers, return_representation=True)\n",
    "        embeddings_rep = embeddings[\"representations\"][layers][:, 1 : -1] #B*(T+2)*E -> B*T*E\n",
    "\n",
    "        for i, layer in enumerate(self.convtransformer_decoder):\n",
    "            x_o, attn = layer(x=embeddings_rep, self_attn_padding_mask=self_attn_padding_mask)  #tokens: B*T*E, x_o: B*T*E\n",
    "\n",
    "        x = torch.flip(x_o, dims=[1])  # Reverse along the sequence length dimension\n",
    "        # Select frames corresponding to frame 1, frame 2, and frame 3\n",
    "        frame_1 = x[:, 0::3, :]\n",
    "        frame_2 = x[:, 1::3, :]\n",
    "        frame_3 = x[:, 2::3, :]\n",
    "        # 全局最大池化\n",
    "        frame_1_max = torch.max(frame_1, dim=1)[0]  # B*C\n",
    "        frame_2_max = torch.max(frame_2, dim=1)[0]  # B*C\n",
    "        frame_3_max = torch.max(frame_3, dim=1)[0]  # B*C\n",
    "        # 扩展 self_attn_padding_mask 的维度以匹配特征张量\n",
    "        mask_expanded = ~self_attn_padding_mask.unsqueeze(2)  # (batch_size, seq_len, 1)，True 表示有效数据\n",
    "        # 计算有效位置的均值池化\n",
    "        def masked_mean(frame, mask):\n",
    "            frame_sum = torch.sum(frame * mask, dim=1)\n",
    "            mask_sum = torch.sum(mask, dim=1) + 1e-8  # 避免除零\n",
    "            return frame_sum / mask_sum\n",
    "        # 全局均值池化\n",
    "        frame_1_avg = masked_mean(frame_1, mask_expanded[:, 0::3, :])\n",
    "        frame_2_avg = masked_mean(frame_2, mask_expanded[:, 1::3, :])\n",
    "        frame_3_avg = masked_mean(frame_3, mask_expanded[:, 2::3, :])\n",
    "        # 将池化后的张量拼接为一个张量\n",
    "        pooled_output = torch.cat([frame_1_max, frame_1_avg, frame_2_max, frame_2_avg, frame_3_max, frame_3_avg], dim=1)  # B*(6*C)\n",
    "        # 线性层处理实验指示符\n",
    "        experiment_output = self.experiment_dense(experiment_indicator)\n",
    "        x_pooled = self.flatten(pooled_output)\n",
    "\n",
    "        o_linear = self.linear(x_pooled) + experiment_output #将池化输出与实验信息拼接\n",
    "        o_linear_2 = self.linear_2(o_linear)\n",
    "        o_linear_3 = self.linear_3(o_linear_2)\n",
    "\n",
    "        o_relu = self.relu(o_linear_3)\n",
    "        o_dropout = self.dropout(o_relu)\n",
    "        o = self.output(o_dropout)  # B*1\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x,y):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    return r_value**2\n",
    "\n",
    "def performances(label, pred):\n",
    "    label, pred = list(label), list(pred)\n",
    "    r = r2(label, pred)\n",
    "    R2 = r2_score(label, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(label, pred))\n",
    "    mae = mean_absolute_error(label, pred)\n",
    "    try:\n",
    "        pearson_r = pearsonr(label, pred)[0]\n",
    "    except:\n",
    "        pearson_r = -1e-9\n",
    "    try:\n",
    "        sp_cor = spearmanr(label, pred)[0]\n",
    "    except:\n",
    "        sp_cor = -1e-9\n",
    "    print(f'r-squared = {r:.4f} | pearson r = {pearson_r:.4f} | spearman R = {sp_cor:.4f} | R-squared = {R2:.4f} | RMSE = {rmse:.4f} | MAE = {mae:.4f}')\n",
    "    return [r, pearson_r, sp_cor, R2, rmse, mae]\n",
    "\n",
    "def performances_to_pd(performances_list):\n",
    "    performances_pd = pd.DataFrame(performances_list, index = ['r2', 'PearsonR', 'SpearmanR', 'R2', 'RMSE', 'MAE']).T\n",
    "    return performances_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_dataloader(e_data, obj_col, lab_col, batch_toks=8192*4, mask_prob = 0.0):\n",
    "    dataset = FastaBatchedDataset(e_data.loc[:,obj_col], e_data.loc[:, lab_col], mask_prob = mask_prob)\n",
    "    batches = dataset.get_batch_indices(toks_per_batch=batch_toks, extra_toks_per_seq=2)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                            collate_fn=alphabet.get_batch_converter(), \n",
    "                                            batch_sampler=batches, \n",
    "                                            shuffle = False)\n",
    "    print(f\"{len(dataset)} sequences\")\n",
    "    return dataset, dataloader, batches\n",
    "\n",
    "def get_experiment_indicator_for_batch(data_combine, batch_idx):\n",
    "    # 从 train_combine 中获取对应 batch 的 experiment_indicator\n",
    "    batch_experiment_indicators = data_combine.iloc[batch_idx]['experiment_indicator'].values.tolist()\n",
    "    # 转换为 tensor\n",
    "    experiment_indicator_tensor = torch.tensor(batch_experiment_indicators, dtype=torch.float32).to(device)\n",
    "    return experiment_indicator_tensor\n",
    "\n",
    "def shuffle_data_fn(in_data):\n",
    "    # 使用 sample(frac=1) 来打乱数据集顺序\n",
    "    shuffle_data = in_data.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_dataloader, train_shuffle_combine, train_shuffle_batch, model, epoch):        \n",
    "    model.train()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    \n",
    "    for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        toks = toks.to(device)\n",
    "        padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "        labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "        experiment_indicator_tensor = get_experiment_indicator_for_batch(train_shuffle_combine, train_shuffle_batch[i])\n",
    "\n",
    "        outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.cpu().detach())\n",
    "\n",
    "        y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "        y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "        y_pred_list.extend(y_pred)\n",
    "        \n",
    "    loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "    print(f'Train: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "    \n",
    "    metrics = performances(y_true_list, y_pred_list)\n",
    "    return metrics, loss_epoch\n",
    "\n",
    "def eval_step(test_dataloader, test_combine, test_batch, model, epoch):\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list, loss_list = [], [], []\n",
    "    strs_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (labels, strs, masked_strs, toks, masked_toks, _) in enumerate(tqdm(test_dataloader)):\n",
    "            strs_list.extend(strs)\n",
    "            toks = toks.to(device)\n",
    "            padding_mask = toks.eq(alphabet.padding_idx)[:, 1:-1]\n",
    "            labels = torch.FloatTensor(labels).to(device).reshape(-1, 1)\n",
    "            experiment_indicator_tensor = get_experiment_indicator_for_batch(test_combine, test_batch[i])\n",
    "            \n",
    "            outputs= model(toks, experiment_indicator_tensor, self_attn_padding_mask=padding_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.cpu().detach())\n",
    "\n",
    "            y_pred = outputs.reshape(-1).cpu().detach().tolist()\n",
    "            y_true_list.extend(labels.cpu().reshape(-1).tolist())\n",
    "            y_pred_list.extend(y_pred)\n",
    "        \n",
    "        loss_epoch = float(torch.Tensor(loss_list).mean())\n",
    "        print(f'Test: Epoch-{epoch}/{num_epochs} | Loss = {loss_epoch:.4f} | ', end = '')\n",
    "        metrics = performances(y_true_list, y_pred_list)\n",
    "        e_pred = pd.DataFrame([strs_list, y_true_list, y_pred_list], index = ['utr', 'y_true', 'y_pred']).T\n",
    "        \n",
    "    return metrics, loss_epoch, e_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====Load Data====')\n",
    "train_data_50_random = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/train_test/4.1_train_data_GSM3130435_egfp_unmod_1_BiologyFeatures.csv')\n",
    "train_data_vary_random = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/train_test/VaryLengthRandomTrain_sequence.csv')\n",
    "test_data_50_random = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/train_test/4.1_test_data_GSM3130435_egfp_unmod_1.csv')\n",
    "test_data_vary_random = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/train_test/VaryLengthRandomTest_sequence_num7600.csv')\n",
    "eval_data_vary_human = pd.read_csv(f'/devdata/pansc/github/UTR_Insight/Data/train_test/VaryLengthHumanAll_sequence_num15555.csv') #/devdata/pansc/github/UTR-LM/Data/IndependentTest_VaryLength_Sample/VaryLengthHumanTest_sequence_num7600.csv\n",
    "# /devdata/pansc/github/UTR-LM/Data/IndependentTest_VaryLength_Sample/VaryLengthHumanTest_sequence_num7600.csv\n",
    "\n",
    "# train data\n",
    "# 在utr列左侧加50个<pad>\n",
    "train_data_50_random['utr_100'] = '<pad>'*50 + train_data_50_random['utr']\n",
    "test_data_50_random['utr_100'] = '<pad>'*50 + test_data_50_random['utr']\n",
    "# 选择 'rl' 和 'utr' 列\n",
    "train_data_50_selected = train_data_50_random[['rl', 'utr_100']]\n",
    "train_data_vary_selected = train_data_vary_random[['rl', 'utr_100']]\n",
    "# 为 train_data_50_selected 和 train_data_vary_selected 添加实验指示符\n",
    "train_data_50_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(train_data_50_selected)\n",
    "train_data_vary_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(train_data_vary_selected)\n",
    "# 合并两个数据集\n",
    "train_combine = pd.concat([train_data_50_selected, train_data_vary_selected], ignore_index=True)\n",
    "\n",
    "# test data\n",
    "# 选择 'rl' 和 'utr' 列\n",
    "test_data_50_selected = test_data_50_random[['rl', 'utr_100']]\n",
    "test_data_vary_selected = test_data_vary_random[['rl', 'utr_100']]\n",
    "# 为 test_data_50_selected, test_data_vary_selected, eval_human_selected添加实验指示符\n",
    "test_data_50_selected.loc[:, 'experiment_indicator'] = [[1, 0]] * len(test_data_50_selected)\n",
    "test_data_vary_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(test_data_vary_selected)\n",
    "# 合并两个数据集\n",
    "test_combine = pd.concat([test_data_50_selected, test_data_vary_selected], ignore_index=True)\n",
    "\n",
    "# eval data\n",
    "eval_human_selected = eval_data_vary_human[['rl', 'utr_100']]\n",
    "eval_human_selected.loc[:, 'experiment_indicator'] = [[0, 1]] * len(eval_human_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_50_random))\n",
    "print(len(train_data_vary_random))\n",
    "print(len(test_data_50_random))\n",
    "print(len(test_data_vary_random))\n",
    "print(len(eval_data_vary_human))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = Alphabet(mask_prob = 0.0, standard_toks = 'AGCT')\n",
    "print(alphabet.tok_to_idx)\n",
    "assert alphabet.tok_to_idx == {'<pad>': 0, '<eos>': 1, '<unk>': 2, 'A': 3, 'G': 4, 'C': 5, 'T': 6, '<cls>': 7, '<mask>': 8, '<sep>': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_data = shuffle_data_fn(train_combine)\n",
    "train_shuffle_dataset,  train_shuffle_dataloader, train_shuffle_batch = generate_dataset_dataloader(train_shuffle_data, 'rl', 'utr_100', mask_prob = 0.0)\n",
    "test_dataset, test_dataloader, test_batch = generate_dataset_dataloader(test_combine, 'rl', 'utr_100', mask_prob = 0.0)\n",
    "\n",
    "esm2_modelfile = '/devdata/pansc/github/UTR_Insight/Model/utr_lm/ESM2SISS_FS4.22_fiveSpeciesCao_6layers_16heads_128embedsize_4096batchToks_lr1e-05_supervisedweight1.0_structureweight1.0_MLMLossMin_epoch115.pkl'\n",
    "model = ConvTransformerPredictor(alphabet).to(device)\n",
    "state_dict = torch.load(esm2_modelfile, map_location=device)\n",
    "model.esm2.load_state_dict({k.replace('module.', ''):v for k,v in state_dict.items()})\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "learning_rate = 1e-4 #1e-4, 1e-05\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=learning_rate,\n",
    "#     betas = (0.9, 0.999),\n",
    "#     eps = 1e-08\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate, \n",
    "    momentum=0.9,\n",
    "    weight_decay = 1e-4)\n",
    "\n",
    "# criterion = torch.nn.MSELoss() #torch.nn.HuberLoss()\n",
    "criterion = torch.nn.HuberLoss()\n",
    "\n",
    "loss_best, ep_best, r2_best = np.inf, -1, -1\n",
    "loss_train_dict, loss_test_dict = dict(), dict()\n",
    "\n",
    "metrics_train_dict = dict()\n",
    "metrics_test_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定训练信息\n",
    "train_info = 'train_info'\n",
    "folder_path = f\"/devdata/pansc/github/UTR_Insight/{train_info}/\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"文件夹已创建: {folder_path}\")\n",
    "else:\n",
    "    print(f\"文件夹已存在: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    metrics_train, loss_train = train_step(train_shuffle_dataloader, train_shuffle_data, train_shuffle_batch, model, epoch)\n",
    "    loss_train_dict[epoch] = loss_train\n",
    "    metrics_train_dict[epoch] = metrics_train\n",
    "\n",
    "\n",
    "    metrics_test, loss_test, _ = eval_step(test_dataloader, test_combine, test_batch, model, epoch)\n",
    "    loss_test_dict[epoch] = loss_test\n",
    "    metrics_test_dict[epoch] = metrics_test\n",
    "\n",
    "    if metrics_test[0] > r2_best: \n",
    "        path_saver = f'/devdata/pansc/github/UTR_Insight/{train_info}/model_epoch{epoch}.pkl'\n",
    "        r2_best, ep_best = metrics_test[0], epoch\n",
    "        torch.save(model.eval().state_dict(), path_saver)\n",
    "        print(f'****Saving model in {path_saver}: Best epoch = {ep_best} | Train Loss = {loss_train:.4f} |  Val Loss = {loss_test:.4f} | R2_best = {r2_best:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_df = pd.DataFrame(loss_train_dict,\n",
    "                             index = ['train_loss']).T\n",
    "loss_test_df = pd.DataFrame(loss_test_dict,\n",
    "                            index = ['test_loss']).T\n",
    "\n",
    "metrics_train_df = pd.DataFrame(metrics_train_dict, \n",
    "                                index = [\n",
    "                                        'Train_r2', 'Train_PearsonR', 'Train_SpearmanR', 'Train_R2', 'Train_RMSE', 'Train_MAE'\n",
    "                                        ]).T\n",
    "\n",
    "metrics_test_df = pd.DataFrame(metrics_test_dict, \n",
    "                                index = [\n",
    "                                        'test_r2', 'test_PearsonR', 'test_SpearmanR', 'test_R2', 'test_RMSE', 'test_MAE'\n",
    "                                        ]).T\n",
    "\n",
    "loss_train_df.to_csv(f'/devdata/pansc/github/UTR_Insight/{train_info}/train_loss.csv')\n",
    "loss_test_df.to_csv(f'/devdata/pansc/github/UTR_Insight/{train_info}/test_loss.csv')\n",
    "\n",
    "metrics_train_df.to_csv(f'/devdata/pansc/github/UTR_Insight/{train_info}/train_metrics.csv', index = True)\n",
    "metrics_test_df.to_csv(f'/devdata/pansc/github/UTR_Insight/{train_info}/test_metrics.csv', index = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
